{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8502365-861a-418a-aed0-360377630c23",
   "metadata": {},
   "source": [
    "# Lab 8\n",
    "\n",
    "In this lab, we build a convolutional neural netowrk for classification of CIFAR-10 images. \n",
    "For more information about CIFAR-10 and CIFAR-100 datasets visit: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb365d-c2ba-47b8-87c6-754983b2001a",
   "metadata": {},
   "source": [
    "The first step is to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfa01c8d-8d1b-4a65-a09b-b64588fad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required libraries\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets.utils import download_url\n",
    "import tarfile\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "### if you get trouble downloading the CIFAR-10 dataset from pytorch due to SSL issues, uncomment the following two lines\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21860080-68b1-4ef1-9ef0-d360f5f47bcc",
   "metadata": {},
   "source": [
    "Noe we set up the computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31aec9d5-7267-4717-9e74-1459ecedaa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n",
      "Device is cpu.\n"
     ]
    }
   ],
   "source": [
    "### Computing Environment Setup \n",
    "if torch.cuda.is_available():\n",
    "    num_GPUs = torch.cuda.device_count()\n",
    "    print(f'Available GPUs: {num_GPUs}')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = 0 \n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.set_device(int(configs['gpu']))\n",
    "else:\n",
    "    print('GPU is not available.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    num_GPUs = 0\n",
    "print(f'Device is {device}.')\n",
    "\n",
    "### Fix random seeds for reproducibility\n",
    "SEED = 12345\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)  \n",
    "\n",
    "### Set number of workers for loading data\n",
    "# Interesting read: https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5\n",
    "if num_GPUs == 0:\n",
    "    num_workers = 8\n",
    "else:\n",
    "    num_workers = 4 * num_GPUs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc59b71-574a-4d7d-be18-20d0061b6303",
   "metadata": {},
   "source": [
    "Setting up the hyperparameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e99da531-5392-478e-b06b-d05ede8dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters and Hyperparameters Setup\n",
    "batch_size = 8 \n",
    "valid_size = 0.1\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "print_every_other = 2000 # print training loss every n batch\n",
    "save_model = True # save the trained model\n",
    "model_save_path = './cifar_net.pth' # path and file name to save the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cac7a-f1d5-4cb0-b91a-793028df02c4",
   "metadata": {},
   "source": [
    "Loading the dataset using torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08536872-764a-4faa-87d6-39cc8c99bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "45000 5000\n"
     ]
    }
   ],
   "source": [
    "### Loading the data\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Interesting read: https://stackoverflow.com/questions/65676151/how-does-torchvision-transforms-normalize-operate\n",
    "    ])\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "validset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=valid_transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=valid_transform)\n",
    "\n",
    "num_train = len(trainset)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n",
    "                                          shuffle=False, sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(len(trainloader.sampler), len(validloader.sampler))\n",
    "\n",
    "target_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ec140-0675-413f-ba47-7aa2107154f4",
   "metadata": {},
   "source": [
    "Let's see some samples from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e109a45-a4a5-4413-b424-4bc07201f513",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Select a batch of images\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m## Show images and labels\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "### Visualization of images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "## Select a batch of images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "## Show images and labels\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join(f'{target_names[labels[j]]:5s}' for j in range(batch_size))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a816811-49dc-4d47-b076-6114e7f6c28e",
   "metadata": {},
   "source": [
    "It iss time to build our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61ce6867-7819-416b-866a-7e08dedc1a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc929ed4-6f2d-4c29-a1fd-f1beab28b157",
   "metadata": {},
   "source": [
    "Need to set up the loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de132618-fbff-4ac9-a221-b8221d43a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up the training environment\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8d460dd-0e86-424e-b0bb-ab6f418a025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(net, validloader, criterion):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    counter = 0\n",
    "    total_loss = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels).item()\n",
    "            total_loss += loss\n",
    "            counter+=1\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = correct / total\n",
    "        loss = total_loss/counter\n",
    "    return loss, acc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cf925ce-7741-4bf9-a25a-8300e8aae09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      " Epoch: 1, Iteration:  2000, Train Loss: 2.070, Train Accuracy: 0.25\n",
      " Epoch: 1, Iteration:  4000, Train Loss: 1.948, Train Accuracy: 0.375\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4) to match target batch_size (8).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mprint_every_other\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mcount_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m             running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 14\u001b[0m, in \u001b[0;36mmodel_validation\u001b[0;34m(net, validloader, criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# calculate outputs by running images through the network\u001b[39;00m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(images)\n\u001b[0;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     16\u001b[0m counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (4) to match target batch_size (8)."
     ]
    }
   ],
   "source": [
    "### Training \n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    count_samples = 0\n",
    "    \n",
    "    for iter, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data # data is a list of [inputs, labels]\n",
    "        ## Copying the data to device\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        ## Zero-ing the grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Feed forward\n",
    "        outputs = net(inputs)\n",
    "        ## Compupting loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        ## Compute the gradients\n",
    "        loss.backward()\n",
    "        ## Perform a single optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Collect and print training statistics\n",
    "        running_loss += loss.item()\n",
    "        if iter % print_every_other == print_every_other-1:    # print every 2000 mini-batches\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()   \n",
    "            count_samples = len(labels)\n",
    "            print(f' Epoch: {epoch + 1}, Iteration: {iter + 1:5d}, Train Loss: {running_loss / print_every_other:.3f}, Train Accuracy: {correct / count_samples}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "    valid_loss, valid_acc = model_validation(net, validloader, criterion)\n",
    "    print(f' Epoch: {epoch + 1}, Validation Loss: {valid_loss}, Validation Accuracy: {valid_acc}')\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e1580-6a8f-4642-b3cd-1c7accf2ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the trained model\n",
    "if save_model:\n",
    "    torch.save(net.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f834155-8609-4754-b70e-925ea8b09f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(model_save_path):\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(model_save_path))\n",
    "    print('Loaded saved model.')\n",
    "else:\n",
    "    print('Saved model does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a01d78-36c6-4509-836b-98e31076cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0\n",
      "8 0\n",
      "12 0\n",
      "16 0\n",
      "20 0\n",
      "24 0\n",
      "28 0\n",
      "32 0\n",
      "36 1\n",
      "40 1\n",
      "44 1\n",
      "48 1\n",
      "52 1\n",
      "56 1\n",
      "60 1\n",
      "64 2\n",
      "68 2\n",
      "72 3\n",
      "76 3\n",
      "80 3\n",
      "84 3\n",
      "88 6\n",
      "92 7\n",
      "96 7\n",
      "100 7\n",
      "104 9\n",
      "108 9\n",
      "112 9\n",
      "116 10\n",
      "120 11\n",
      "124 11\n",
      "128 12\n",
      "132 12\n",
      "136 12\n",
      "140 13\n",
      "144 15\n",
      "148 15\n",
      "152 15\n",
      "156 15\n",
      "160 16\n",
      "164 17\n",
      "168 17\n",
      "172 17\n",
      "176 17\n",
      "180 17\n",
      "184 19\n",
      "188 20\n",
      "192 20\n",
      "196 20\n",
      "200 20\n",
      "204 20\n",
      "208 20\n",
      "212 20\n",
      "216 20\n",
      "220 20\n",
      "224 20\n",
      "228 20\n",
      "232 20\n",
      "236 20\n",
      "240 20\n",
      "244 20\n",
      "248 20\n",
      "252 22\n",
      "256 22\n",
      "260 22\n",
      "264 22\n",
      "268 23\n",
      "272 25\n",
      "276 25\n",
      "280 26\n",
      "284 26\n",
      "288 26\n",
      "292 26\n",
      "296 26\n",
      "300 26\n",
      "304 26\n",
      "308 26\n",
      "312 26\n",
      "316 26\n",
      "320 26\n",
      "324 29\n",
      "328 29\n",
      "332 29\n",
      "336 29\n",
      "340 29\n",
      "344 31\n",
      "348 32\n",
      "352 32\n",
      "356 33\n",
      "360 33\n",
      "364 34\n",
      "368 35\n",
      "372 35\n",
      "376 36\n",
      "380 36\n",
      "384 36\n",
      "388 37\n",
      "392 39\n",
      "396 39\n",
      "400 41\n",
      "404 41\n",
      "408 41\n",
      "412 41\n",
      "416 41\n",
      "420 41\n",
      "424 42\n",
      "428 43\n",
      "432 44\n",
      "436 44\n",
      "440 46\n",
      "444 46\n",
      "448 46\n",
      "452 48\n",
      "456 48\n",
      "460 49\n",
      "464 49\n",
      "468 50\n",
      "472 50\n",
      "476 50\n",
      "480 50\n",
      "484 50\n",
      "488 50\n",
      "492 50\n",
      "496 50\n",
      "500 51\n",
      "504 51\n",
      "508 51\n",
      "512 51\n",
      "516 52\n",
      "520 52\n",
      "524 52\n",
      "528 53\n",
      "532 53\n",
      "536 54\n",
      "540 54\n",
      "544 54\n",
      "548 54\n",
      "552 56\n",
      "556 57\n",
      "560 57\n",
      "564 57\n",
      "568 58\n",
      "572 58\n",
      "576 58\n",
      "580 58\n",
      "584 58\n",
      "588 58\n",
      "592 58\n",
      "596 59\n",
      "600 60\n",
      "604 61\n",
      "608 62\n",
      "612 62\n",
      "616 62\n",
      "620 63\n",
      "624 63\n",
      "628 63\n",
      "632 64\n",
      "636 64\n",
      "640 64\n",
      "644 64\n",
      "648 64\n",
      "652 65\n",
      "656 65\n",
      "660 65\n",
      "664 65\n",
      "668 65\n",
      "672 66\n",
      "676 67\n",
      "680 67\n",
      "684 67\n",
      "688 67\n",
      "692 67\n",
      "696 69\n",
      "700 69\n",
      "704 69\n",
      "708 70\n",
      "712 70\n",
      "716 71\n",
      "720 71\n",
      "724 71\n",
      "728 72\n",
      "732 73\n",
      "736 73\n",
      "740 74\n",
      "744 75\n",
      "748 75\n",
      "752 77\n",
      "756 78\n",
      "760 79\n",
      "764 79\n",
      "768 79\n",
      "772 79\n",
      "776 79\n",
      "780 80\n",
      "784 80\n",
      "788 80\n",
      "792 80\n",
      "796 80\n",
      "800 80\n",
      "804 80\n",
      "808 80\n",
      "812 81\n",
      "816 81\n",
      "820 83\n",
      "824 83\n",
      "828 83\n",
      "832 84\n",
      "836 85\n",
      "840 85\n",
      "844 85\n",
      "848 86\n",
      "852 87\n",
      "856 87\n",
      "860 87\n",
      "864 88\n",
      "868 88\n",
      "872 88\n",
      "876 88\n",
      "880 88\n",
      "884 88\n",
      "888 88\n",
      "892 88\n",
      "896 88\n",
      "900 88\n",
      "904 89\n",
      "908 89\n",
      "912 90\n",
      "916 90\n",
      "920 90\n",
      "924 90\n",
      "928 91\n",
      "932 91\n",
      "936 91\n",
      "940 93\n",
      "944 93\n",
      "948 93\n",
      "952 93\n",
      "956 93\n",
      "960 93\n",
      "964 93\n",
      "968 93\n",
      "972 93\n",
      "976 94\n",
      "980 94\n",
      "984 95\n",
      "988 95\n",
      "992 96\n",
      "996 97\n",
      "1000 97\n",
      "1004 97\n",
      "1008 97\n",
      "1012 97\n",
      "1016 97\n",
      "1020 97\n",
      "1024 97\n",
      "1028 98\n",
      "1032 99\n",
      "1036 99\n",
      "1040 100\n",
      "1044 101\n",
      "1048 103\n",
      "1052 103\n",
      "1056 103\n",
      "1060 103\n",
      "1064 103\n",
      "1068 103\n",
      "1072 103\n",
      "1076 103\n",
      "1080 103\n",
      "1084 103\n",
      "1088 103\n",
      "1092 103\n",
      "1096 103\n",
      "1100 104\n",
      "1104 104\n",
      "1108 104\n",
      "1112 106\n",
      "1116 106\n",
      "1120 107\n",
      "1124 108\n",
      "1128 109\n",
      "1132 109\n",
      "1136 110\n",
      "1140 110\n",
      "1144 110\n",
      "1148 111\n",
      "1152 113\n",
      "1156 113\n",
      "1160 113\n",
      "1164 114\n",
      "1168 114\n",
      "1172 114\n",
      "1176 115\n",
      "1180 116\n",
      "1184 116\n",
      "1188 116\n",
      "1192 116\n",
      "1196 117\n",
      "1200 118\n",
      "1204 119\n",
      "1208 119\n",
      "1212 119\n",
      "1216 119\n",
      "1220 120\n",
      "1224 120\n",
      "1228 121\n",
      "1232 121\n",
      "1236 121\n",
      "1240 121\n",
      "1244 122\n",
      "1248 123\n",
      "1252 124\n",
      "1256 125\n",
      "1260 125\n",
      "1264 125\n",
      "1268 125\n",
      "1272 125\n",
      "1276 126\n",
      "1280 127\n",
      "1284 127\n",
      "1288 127\n",
      "1292 127\n",
      "1296 127\n",
      "1300 127\n",
      "1304 127\n",
      "1308 127\n",
      "1312 127\n",
      "1316 127\n",
      "1320 127\n",
      "1324 127\n",
      "1328 127\n",
      "1332 129\n",
      "1336 129\n",
      "1340 130\n",
      "1344 130\n",
      "1348 130\n",
      "1352 131\n",
      "1356 132\n",
      "1360 133\n",
      "1364 133\n",
      "1368 133\n",
      "1372 133\n",
      "1376 134\n",
      "1380 134\n",
      "1384 135\n",
      "1388 136\n",
      "1392 137\n",
      "1396 139\n",
      "1400 139\n",
      "1404 139\n",
      "1408 139\n",
      "1412 140\n",
      "1416 140\n",
      "1420 140\n",
      "1424 140\n",
      "1428 140\n",
      "1432 140\n",
      "1436 140\n",
      "1440 140\n",
      "1444 140\n",
      "1448 140\n",
      "1452 141\n",
      "1456 142\n",
      "1460 143\n",
      "1464 143\n",
      "1468 143\n",
      "1472 143\n",
      "1476 143\n",
      "1480 143\n",
      "1484 143\n",
      "1488 143\n",
      "1492 143\n",
      "1496 143\n",
      "1500 144\n",
      "1504 145\n",
      "1508 145\n",
      "1512 145\n",
      "1516 145\n",
      "1520 145\n",
      "1524 145\n",
      "1528 145\n",
      "1532 146\n",
      "1536 146\n",
      "1540 146\n",
      "1544 146\n",
      "1548 146\n",
      "1552 146\n",
      "1556 146\n",
      "1560 146\n",
      "1564 147\n",
      "1568 147\n",
      "1572 147\n",
      "1576 147\n",
      "1580 148\n",
      "1584 148\n",
      "1588 148\n",
      "1592 148\n",
      "1596 149\n",
      "1600 149\n",
      "1604 150\n",
      "1608 151\n",
      "1612 153\n",
      "1616 153\n",
      "1620 153\n",
      "1624 153\n",
      "1628 153\n",
      "1632 153\n",
      "1636 153\n",
      "1640 154\n",
      "1644 154\n",
      "1648 155\n",
      "1652 156\n",
      "1656 156\n",
      "1660 158\n",
      "1664 158\n",
      "1668 158\n",
      "1672 158\n",
      "1676 158\n",
      "1680 159\n",
      "1684 160\n",
      "1688 160\n",
      "1692 161\n",
      "1696 161\n",
      "1700 161\n",
      "1704 162\n",
      "1708 162\n",
      "1712 162\n",
      "1716 163\n",
      "1720 163\n",
      "1724 163\n",
      "1728 163\n",
      "1732 163\n",
      "1736 163\n",
      "1740 163\n",
      "1744 163\n",
      "1748 165\n",
      "1752 165\n",
      "1756 165\n",
      "1760 167\n",
      "1764 167\n",
      "1768 167\n",
      "1772 168\n",
      "1776 170\n",
      "1780 171\n",
      "1784 171\n",
      "1788 172\n",
      "1792 172\n",
      "1796 173\n",
      "1800 173\n",
      "1804 174\n",
      "1808 174\n",
      "1812 174\n",
      "1816 174\n",
      "1820 174\n",
      "1824 174\n",
      "1828 174\n",
      "1832 174\n",
      "1836 174\n",
      "1840 174\n",
      "1844 174\n",
      "1848 175\n",
      "1852 175\n",
      "1856 176\n",
      "1860 177\n",
      "1864 177\n",
      "1868 177\n",
      "1872 180\n",
      "1876 180\n",
      "1880 181\n",
      "1884 182\n",
      "1888 184\n",
      "1892 185\n",
      "1896 185\n",
      "1900 185\n",
      "1904 186\n",
      "1908 186\n",
      "1912 186\n",
      "1916 186\n",
      "1920 187\n",
      "1924 188\n",
      "1928 188\n",
      "1932 188\n",
      "1936 188\n",
      "1940 188\n",
      "1944 189\n",
      "1948 190\n",
      "1952 190\n",
      "1956 191\n",
      "1960 191\n",
      "1964 192\n",
      "1968 192\n",
      "1972 192\n",
      "1976 192\n",
      "1980 194\n",
      "1984 194\n",
      "1988 194\n",
      "1992 194\n",
      "1996 194\n",
      "2000 194\n",
      "2004 195\n",
      "2008 196\n",
      "2012 196\n",
      "2016 196\n",
      "2020 197\n",
      "2024 197\n",
      "2028 197\n",
      "2032 197\n",
      "2036 197\n",
      "2040 199\n",
      "2044 199\n",
      "2048 199\n",
      "2052 199\n",
      "2056 199\n",
      "2060 200\n",
      "2064 201\n",
      "2068 202\n",
      "2072 203\n",
      "2076 204\n",
      "2080 205\n",
      "2084 206\n",
      "2088 206\n",
      "2092 207\n",
      "2096 207\n",
      "2100 208\n",
      "2104 209\n",
      "2108 210\n",
      "2112 210\n",
      "2116 211\n",
      "2120 211\n",
      "2124 211\n",
      "2128 211\n",
      "2132 211\n",
      "2136 212\n",
      "2140 212\n",
      "2144 212\n",
      "2148 213\n",
      "2152 213\n",
      "2156 213\n",
      "2160 213\n",
      "2164 214\n",
      "2168 214\n",
      "2172 215\n",
      "2176 216\n",
      "2180 216\n",
      "2184 216\n",
      "2188 216\n",
      "2192 216\n",
      "2196 216\n",
      "2200 216\n",
      "2204 217\n",
      "2208 219\n",
      "2212 220\n",
      "2216 221\n",
      "2220 222\n",
      "2224 222\n",
      "2228 223\n",
      "2232 224\n",
      "2236 224\n",
      "2240 224\n",
      "2244 226\n",
      "2248 226\n",
      "2252 226\n",
      "2256 226\n",
      "2260 226\n",
      "2264 227\n",
      "2268 227\n",
      "2272 228\n",
      "2276 229\n",
      "2280 230\n",
      "2284 231\n",
      "2288 231\n",
      "2292 231\n",
      "2296 231\n",
      "2300 231\n",
      "2304 231\n",
      "2308 231\n",
      "2312 231\n",
      "2316 232\n",
      "2320 234\n",
      "2324 234\n",
      "2328 234\n",
      "2332 234\n",
      "2336 235\n",
      "2340 235\n",
      "2344 235\n",
      "2348 235\n",
      "2352 236\n",
      "2356 237\n",
      "2360 237\n",
      "2364 237\n",
      "2368 237\n",
      "2372 237\n",
      "2376 237\n",
      "2380 237\n",
      "2384 238\n",
      "2388 238\n",
      "2392 238\n",
      "2396 238\n",
      "2400 239\n",
      "2404 239\n",
      "2408 239\n",
      "2412 240\n",
      "2416 240\n",
      "2420 240\n",
      "2424 240\n",
      "2428 241\n",
      "2432 241\n",
      "2436 241\n",
      "2440 241\n",
      "2444 241\n",
      "2448 241\n",
      "2452 242\n",
      "2456 243\n",
      "2460 243\n",
      "2464 243\n",
      "2468 245\n",
      "2472 245\n",
      "2476 245\n",
      "2480 245\n",
      "2484 245\n",
      "2488 245\n",
      "2492 245\n",
      "2496 245\n",
      "2500 245\n",
      "2504 245\n",
      "2508 245\n",
      "2512 245\n",
      "2516 246\n",
      "2520 247\n",
      "2524 247\n",
      "2528 248\n",
      "2532 248\n",
      "2536 248\n",
      "2540 248\n",
      "2544 251\n",
      "2548 252\n",
      "2552 252\n",
      "2556 252\n",
      "2560 252\n",
      "2564 252\n",
      "2568 254\n",
      "2572 254\n",
      "2576 255\n",
      "2580 256\n",
      "2584 256\n",
      "2588 256\n",
      "2592 256\n",
      "2596 257\n",
      "2600 257\n",
      "2604 257\n",
      "2608 257\n",
      "2612 259\n",
      "2616 259\n",
      "2620 259\n",
      "2624 259\n",
      "2628 259\n",
      "2632 260\n",
      "2636 260\n",
      "2640 260\n",
      "2644 261\n",
      "2648 261\n",
      "2652 261\n",
      "2656 261\n",
      "2660 261\n",
      "2664 261\n",
      "2668 261\n",
      "2672 261\n",
      "2676 261\n",
      "2680 261\n",
      "2684 261\n",
      "2688 261\n",
      "2692 263\n",
      "2696 263\n",
      "2700 264\n",
      "2704 264\n",
      "2708 264\n",
      "2712 264\n",
      "2716 264\n",
      "2720 264\n",
      "2724 264\n",
      "2728 264\n",
      "2732 266\n",
      "2736 266\n",
      "2740 266\n",
      "2744 267\n",
      "2748 269\n",
      "2752 269\n",
      "2756 269\n",
      "2760 270\n",
      "2764 271\n",
      "2768 271\n",
      "2772 271\n",
      "2776 273\n",
      "2780 275\n",
      "2784 275\n",
      "2788 276\n",
      "2792 276\n",
      "2796 277\n",
      "2800 277\n",
      "2804 278\n",
      "2808 279\n",
      "2812 280\n",
      "2816 281\n",
      "2820 281\n",
      "2824 282\n",
      "2828 282\n",
      "2832 284\n",
      "2836 284\n",
      "2840 284\n",
      "2844 284\n",
      "2848 285\n",
      "2852 285\n",
      "2856 287\n",
      "2860 288\n",
      "2864 289\n",
      "2868 289\n",
      "2872 290\n",
      "2876 290\n",
      "2880 290\n",
      "2884 290\n",
      "2888 291\n",
      "2892 291\n",
      "2896 291\n",
      "2900 291\n",
      "2904 291\n",
      "2908 291\n",
      "2912 291\n",
      "2916 292\n",
      "2920 293\n",
      "2924 295\n",
      "2928 295\n",
      "2932 295\n",
      "2936 295\n",
      "2940 296\n",
      "2944 296\n",
      "2948 296\n",
      "2952 296\n",
      "2956 296\n",
      "2960 296\n",
      "2964 297\n",
      "2968 297\n",
      "2972 298\n",
      "2976 298\n",
      "2980 299\n",
      "2984 299\n",
      "2988 300\n",
      "2992 302\n",
      "2996 302\n",
      "3000 302\n",
      "3004 302\n",
      "3008 302\n",
      "3012 303\n",
      "3016 303\n",
      "3020 304\n",
      "3024 305\n",
      "3028 305\n",
      "3032 305\n",
      "3036 308\n",
      "3040 308\n",
      "3044 308\n",
      "3048 309\n",
      "3052 309\n",
      "3056 309\n",
      "3060 309\n",
      "3064 309\n",
      "3068 310\n",
      "3072 311\n",
      "3076 311\n",
      "3080 312\n",
      "3084 312\n",
      "3088 312\n",
      "3092 312\n",
      "3096 313\n",
      "3100 313\n",
      "3104 313\n",
      "3108 313\n",
      "3112 313\n",
      "3116 314\n",
      "3120 314\n",
      "3124 314\n",
      "3128 314\n",
      "3132 314\n",
      "3136 314\n",
      "3140 315\n",
      "3144 316\n",
      "3148 317\n",
      "3152 317\n",
      "3156 317\n",
      "3160 317\n",
      "3164 317\n",
      "3168 317\n",
      "3172 317\n",
      "3176 318\n",
      "3180 318\n",
      "3184 319\n",
      "3188 319\n",
      "3192 319\n",
      "3196 319\n",
      "3200 319\n",
      "3204 319\n",
      "3208 319\n",
      "3212 319\n",
      "3216 319\n",
      "3220 319\n",
      "3224 320\n",
      "3228 320\n",
      "3232 321\n",
      "3236 321\n",
      "3240 321\n",
      "3244 321\n",
      "3248 322\n",
      "3252 323\n",
      "3256 323\n",
      "3260 324\n",
      "3264 325\n",
      "3268 326\n",
      "3272 326\n",
      "3276 326\n",
      "3280 326\n",
      "3284 326\n",
      "3288 326\n",
      "3292 326\n",
      "3296 326\n",
      "3300 326\n",
      "3304 326\n",
      "3308 327\n",
      "3312 327\n",
      "3316 328\n",
      "3320 328\n",
      "3324 330\n",
      "3328 330\n",
      "3332 331\n",
      "3336 331\n",
      "3340 331\n",
      "3344 332\n",
      "3348 332\n",
      "3352 332\n",
      "3356 333\n",
      "3360 333\n",
      "3364 333\n",
      "3368 333\n",
      "3372 334\n",
      "3376 334\n",
      "3380 334\n",
      "3384 336\n",
      "3388 338\n",
      "3392 339\n",
      "3396 340\n",
      "3400 341\n",
      "3404 341\n",
      "3408 341\n",
      "3412 342\n",
      "3416 342\n",
      "3420 342\n",
      "3424 343\n",
      "3428 343\n",
      "3432 344\n",
      "3436 344\n",
      "3440 344\n",
      "3444 345\n",
      "3448 345\n",
      "3452 345\n",
      "3456 346\n",
      "3460 346\n",
      "3464 348\n",
      "3468 348\n",
      "3472 349\n",
      "3476 350\n",
      "3480 351\n",
      "3484 352\n",
      "3488 352\n",
      "3492 353\n",
      "3496 353\n",
      "3500 353\n",
      "3504 353\n",
      "3508 354\n",
      "3512 354\n",
      "3516 355\n",
      "3520 355\n",
      "3524 356\n",
      "3528 356\n",
      "3532 356\n",
      "3536 357\n",
      "3540 359\n",
      "3544 359\n",
      "3548 359\n",
      "3552 359\n",
      "3556 359\n",
      "3560 359\n",
      "3564 359\n",
      "3568 359\n",
      "3572 359\n",
      "3576 360\n",
      "3580 360\n",
      "3584 360\n",
      "3588 360\n",
      "3592 360\n",
      "3596 360\n",
      "3600 362\n",
      "3604 362\n",
      "3608 364\n",
      "3612 364\n",
      "3616 365\n",
      "3620 365\n",
      "3624 365\n",
      "3628 366\n",
      "3632 366\n",
      "3636 366\n",
      "3640 366\n",
      "3644 366\n",
      "3648 366\n",
      "3652 366\n",
      "3656 367\n",
      "3660 369\n",
      "3664 369\n",
      "3668 369\n",
      "3672 371\n",
      "3676 371\n",
      "3680 371\n",
      "3684 371\n",
      "3688 371\n",
      "3692 371\n",
      "3696 372\n",
      "3700 372\n",
      "3704 372\n",
      "3708 372\n",
      "3712 374\n",
      "3716 374\n",
      "3720 374\n",
      "3724 375\n",
      "3728 375\n",
      "3732 376\n",
      "3736 376\n",
      "3740 376\n",
      "3744 377\n",
      "3748 377\n",
      "3752 377\n",
      "3756 377\n",
      "3760 378\n",
      "3764 378\n",
      "3768 378\n",
      "3772 378\n",
      "3776 378\n",
      "3780 379\n",
      "3784 379\n",
      "3788 380\n",
      "3792 380\n",
      "3796 380\n",
      "3800 381\n",
      "3804 381\n",
      "3808 381\n",
      "3812 381\n",
      "3816 381\n",
      "3820 383\n",
      "3824 383\n",
      "3828 384\n",
      "3832 384\n",
      "3836 385\n",
      "3840 385\n",
      "3844 385\n",
      "3848 386\n",
      "3852 387\n",
      "3856 388\n",
      "3860 389\n",
      "3864 389\n",
      "3868 390\n",
      "3872 390\n",
      "3876 390\n",
      "3880 390\n",
      "3884 390\n",
      "3888 390\n",
      "3892 390\n",
      "3896 390\n",
      "3900 390\n",
      "3904 390\n",
      "3908 390\n",
      "3912 392\n",
      "3916 392\n",
      "3920 392\n",
      "3924 394\n",
      "3928 394\n",
      "3932 394\n",
      "3936 395\n",
      "3940 395\n",
      "3944 395\n",
      "3948 396\n",
      "3952 397\n",
      "3956 397\n",
      "3960 397\n",
      "3964 398\n",
      "3968 398\n",
      "3972 398\n",
      "3976 398\n",
      "3980 399\n",
      "3984 400\n",
      "3988 400\n",
      "3992 400\n",
      "3996 400\n",
      "4000 401\n",
      "4004 401\n",
      "4008 402\n",
      "4012 402\n",
      "4016 402\n",
      "4020 403\n",
      "4024 403\n",
      "4028 403\n",
      "4032 403\n",
      "4036 404\n",
      "4040 404\n",
      "4044 405\n",
      "4048 405\n",
      "4052 405\n",
      "4056 405\n",
      "4060 405\n",
      "4064 406\n",
      "4068 406\n",
      "4072 408\n",
      "4076 409\n",
      "4080 409\n",
      "4084 409\n",
      "4088 409\n",
      "4092 410\n",
      "4096 410\n",
      "4100 410\n",
      "4104 410\n",
      "4108 411\n",
      "4112 414\n",
      "4116 414\n",
      "4120 415\n",
      "4124 416\n",
      "4128 416\n",
      "4132 416\n",
      "4136 416\n",
      "4140 417\n",
      "4144 417\n",
      "4148 419\n",
      "4152 419\n",
      "4156 420\n",
      "4160 420\n",
      "4164 420\n",
      "4168 420\n",
      "4172 420\n",
      "4176 422\n",
      "4180 422\n",
      "4184 422\n",
      "4188 422\n",
      "4192 422\n",
      "4196 423\n",
      "4200 423\n",
      "4204 423\n",
      "4208 424\n",
      "4212 425\n",
      "4216 425\n",
      "4220 425\n",
      "4224 425\n",
      "4228 426\n",
      "4232 426\n",
      "4236 427\n",
      "4240 427\n",
      "4244 428\n",
      "4248 429\n",
      "4252 429\n",
      "4256 429\n",
      "4260 429\n",
      "4264 429\n",
      "4268 430\n",
      "4272 430\n",
      "4276 431\n",
      "4280 431\n",
      "4284 431\n",
      "4288 432\n",
      "4292 433\n",
      "4296 433\n",
      "4300 433\n",
      "4304 434\n",
      "4308 435\n",
      "4312 435\n",
      "4316 435\n",
      "4320 435\n",
      "4324 436\n",
      "4328 436\n",
      "4332 436\n",
      "4336 436\n",
      "4340 436\n",
      "4344 436\n",
      "4348 436\n",
      "4352 436\n",
      "4356 436\n",
      "4360 436\n",
      "4364 436\n",
      "4368 437\n",
      "4372 437\n",
      "4376 438\n",
      "4380 438\n",
      "4384 438\n",
      "4388 438\n",
      "4392 440\n",
      "4396 440\n",
      "4400 440\n",
      "4404 440\n",
      "4408 441\n",
      "4412 441\n",
      "4416 442\n",
      "4420 442\n",
      "4424 442\n",
      "4428 442\n",
      "4432 442\n",
      "4436 445\n",
      "4440 446\n",
      "4444 447\n",
      "4448 447\n",
      "4452 447\n",
      "4456 448\n",
      "4460 448\n",
      "4464 449\n",
      "4468 450\n",
      "4472 451\n",
      "4476 451\n",
      "4480 451\n",
      "4484 451\n",
      "4488 452\n",
      "4492 452\n",
      "4496 453\n",
      "4500 454\n",
      "4504 455\n",
      "4508 455\n",
      "4512 455\n",
      "4516 455\n",
      "4520 455\n",
      "4524 456\n",
      "4528 457\n",
      "4532 457\n",
      "4536 458\n",
      "4540 458\n",
      "4544 458\n",
      "4548 460\n",
      "4552 460\n",
      "4556 460\n",
      "4560 462\n",
      "4564 462\n",
      "4568 462\n",
      "4572 462\n",
      "4576 463\n",
      "4580 463\n",
      "4584 463\n",
      "4588 463\n",
      "4592 463\n",
      "4596 464\n",
      "4600 464\n",
      "4604 464\n",
      "4608 464\n",
      "4612 464\n",
      "4616 464\n",
      "4620 464\n",
      "4624 465\n",
      "4628 465\n",
      "4632 466\n",
      "4636 466\n",
      "4640 466\n",
      "4644 470\n",
      "4648 471\n",
      "4652 471\n",
      "4656 471\n",
      "4660 471\n",
      "4664 471\n",
      "4668 471\n",
      "4672 471\n",
      "4676 471\n",
      "4680 471\n",
      "4684 471\n",
      "4688 472\n",
      "4692 473\n",
      "4696 473\n",
      "4700 474\n",
      "4704 474\n",
      "4708 475\n",
      "4712 475\n",
      "4716 475\n",
      "4720 475\n",
      "4724 475\n",
      "4728 475\n",
      "4732 476\n",
      "4736 477\n",
      "4740 477\n",
      "4744 477\n",
      "4748 477\n",
      "4752 477\n",
      "4756 477\n",
      "4760 477\n",
      "4764 477\n",
      "4768 477\n",
      "4772 477\n",
      "4776 477\n",
      "4780 478\n",
      "4784 479\n",
      "4788 479\n",
      "4792 479\n",
      "4796 479\n",
      "4800 480\n",
      "4804 480\n",
      "4808 481\n",
      "4812 481\n",
      "4816 481\n",
      "4820 481\n",
      "4824 481\n",
      "4828 481\n",
      "4832 482\n",
      "4836 482\n",
      "4840 482\n",
      "4844 483\n",
      "4848 483\n",
      "4852 484\n",
      "4856 484\n",
      "4860 485\n",
      "4864 485\n",
      "4868 485\n",
      "4872 485\n",
      "4876 485\n",
      "4880 485\n",
      "4884 485\n",
      "4888 485\n",
      "4892 485\n",
      "4896 485\n",
      "4900 485\n",
      "4904 485\n",
      "4908 485\n",
      "4912 485\n",
      "4916 485\n",
      "4920 486\n",
      "4924 487\n",
      "4928 488\n",
      "4932 488\n",
      "4936 488\n",
      "4940 488\n",
      "4944 488\n",
      "4948 488\n",
      "4952 489\n",
      "4956 489\n",
      "4960 489\n",
      "4964 490\n",
      "4968 490\n",
      "4972 491\n",
      "4976 492\n",
      "4980 492\n",
      "4984 492\n",
      "4988 493\n",
      "4992 493\n",
      "4996 493\n",
      "5000 493\n",
      "5004 493\n",
      "5008 495\n",
      "5012 496\n",
      "5016 496\n",
      "5020 496\n",
      "5024 497\n",
      "5028 498\n",
      "5032 498\n",
      "5036 499\n",
      "5040 499\n",
      "5044 500\n",
      "5048 500\n",
      "5052 500\n",
      "5056 500\n",
      "5060 500\n",
      "5064 500\n",
      "5068 500\n",
      "5072 500\n",
      "5076 500\n",
      "5080 500\n",
      "5084 501\n",
      "5088 501\n",
      "5092 501\n",
      "5096 501\n",
      "5100 501\n",
      "5104 502\n",
      "5108 502\n",
      "5112 502\n",
      "5116 502\n",
      "5120 503\n",
      "5124 503\n",
      "5128 503\n",
      "5132 503\n",
      "5136 503\n",
      "5140 503\n",
      "5144 503\n",
      "5148 504\n",
      "5152 504\n",
      "5156 506\n",
      "5160 506\n",
      "5164 506\n",
      "5168 506\n",
      "5172 506\n",
      "5176 506\n",
      "5180 506\n",
      "5184 506\n",
      "5188 506\n",
      "5192 507\n",
      "5196 507\n",
      "5200 508\n",
      "5204 508\n",
      "5208 508\n",
      "5212 508\n",
      "5216 508\n",
      "5220 508\n",
      "5224 508\n",
      "5228 508\n",
      "5232 509\n",
      "5236 509\n",
      "5240 510\n",
      "5244 510\n",
      "5248 511\n",
      "5252 511\n",
      "5256 511\n",
      "5260 512\n",
      "5264 512\n",
      "5268 512\n",
      "5272 513\n",
      "5276 514\n",
      "5280 514\n",
      "5284 514\n",
      "5288 514\n",
      "5292 514\n",
      "5296 514\n",
      "5300 514\n",
      "5304 514\n",
      "5308 514\n",
      "5312 514\n",
      "5316 514\n",
      "5320 514\n",
      "5324 514\n",
      "5328 515\n",
      "5332 515\n",
      "5336 515\n",
      "5340 516\n",
      "5344 517\n",
      "5348 518\n",
      "5352 518\n",
      "5356 518\n",
      "5360 518\n",
      "5364 519\n",
      "5368 519\n",
      "5372 519\n",
      "5376 519\n",
      "5380 520\n",
      "5384 520\n",
      "5388 521\n",
      "5392 521\n",
      "5396 521\n",
      "5400 521\n",
      "5404 521\n",
      "5408 522\n",
      "5412 522\n",
      "5416 522\n",
      "5420 522\n",
      "5424 522\n",
      "5428 522\n",
      "5432 522\n",
      "5436 522\n",
      "5440 522\n",
      "5444 524\n",
      "5448 524\n",
      "5452 524\n",
      "5456 524\n",
      "5460 524\n",
      "5464 524\n",
      "5468 524\n",
      "5472 525\n",
      "5476 526\n",
      "5480 526\n",
      "5484 526\n",
      "5488 526\n",
      "5492 526\n",
      "5496 527\n",
      "5500 527\n",
      "5504 527\n",
      "5508 527\n",
      "5512 528\n",
      "5516 529\n",
      "5520 529\n",
      "5524 530\n",
      "5528 530\n",
      "5532 530\n",
      "5536 531\n",
      "5540 531\n",
      "5544 531\n",
      "5548 531\n",
      "5552 532\n",
      "5556 533\n",
      "5560 534\n",
      "5564 534\n",
      "5568 534\n",
      "5572 534\n",
      "5576 535\n",
      "5580 535\n",
      "5584 535\n",
      "5588 535\n",
      "5592 535\n",
      "5596 535\n",
      "5600 535\n",
      "5604 535\n",
      "5608 535\n",
      "5612 536\n",
      "5616 537\n",
      "5620 538\n",
      "5624 538\n",
      "5628 539\n",
      "5632 539\n",
      "5636 539\n",
      "5640 540\n",
      "5644 541\n",
      "5648 542\n",
      "5652 542\n",
      "5656 544\n",
      "5660 544\n",
      "5664 544\n",
      "5668 544\n",
      "5672 545\n",
      "5676 546\n",
      "5680 546\n",
      "5684 547\n",
      "5688 547\n",
      "5692 547\n",
      "5696 547\n",
      "5700 547\n",
      "5704 547\n",
      "5708 547\n",
      "5712 548\n",
      "5716 551\n",
      "5720 551\n",
      "5724 551\n",
      "5728 552\n",
      "5732 552\n",
      "5736 552\n",
      "5740 553\n",
      "5744 553\n",
      "5748 555\n",
      "5752 556\n",
      "5756 556\n",
      "5760 556\n",
      "5764 557\n",
      "5768 557\n",
      "5772 557\n",
      "5776 557\n",
      "5780 558\n",
      "5784 558\n",
      "5788 558\n",
      "5792 558\n",
      "5796 559\n",
      "5800 560\n",
      "5804 561\n",
      "5808 561\n",
      "5812 561\n",
      "5816 562\n",
      "5820 562\n",
      "5824 562\n",
      "5828 562\n",
      "5832 562\n",
      "5836 563\n",
      "5840 563\n",
      "5844 564\n",
      "5848 565\n",
      "5852 566\n",
      "5856 567\n",
      "5860 567\n",
      "5864 568\n",
      "5868 568\n",
      "5872 568\n",
      "5876 568\n",
      "5880 568\n",
      "5884 568\n",
      "5888 569\n",
      "5892 569\n",
      "5896 569\n",
      "5900 569\n",
      "5904 570\n",
      "5908 570\n",
      "5912 571\n",
      "5916 571\n",
      "5920 571\n",
      "5924 571\n",
      "5928 571\n",
      "5932 571\n",
      "5936 571\n",
      "5940 571\n",
      "5944 572\n",
      "5948 573\n",
      "5952 573\n",
      "5956 573\n",
      "5960 573\n",
      "5964 574\n",
      "5968 574\n",
      "5972 575\n",
      "5976 575\n",
      "5980 575\n",
      "5984 575\n",
      "5988 576\n",
      "5992 577\n",
      "5996 577\n",
      "6000 577\n",
      "6004 578\n",
      "6008 578\n",
      "6012 578\n",
      "6016 578\n",
      "6020 578\n",
      "6024 578\n",
      "6028 580\n",
      "6032 580\n",
      "6036 580\n",
      "6040 581\n",
      "6044 582\n",
      "6048 583\n",
      "6052 584\n",
      "6056 584\n",
      "6060 584\n",
      "6064 584\n",
      "6068 586\n",
      "6072 586\n",
      "6076 587\n",
      "6080 589\n",
      "6084 589\n",
      "6088 589\n",
      "6092 589\n",
      "6096 589\n",
      "6100 591\n",
      "6104 591\n",
      "6108 592\n",
      "6112 593\n",
      "6116 593\n",
      "6120 593\n",
      "6124 593\n",
      "6128 593\n",
      "6132 594\n",
      "6136 595\n",
      "6140 595\n",
      "6144 595\n",
      "6148 595\n",
      "6152 595\n",
      "6156 595\n",
      "6160 595\n",
      "6164 597\n",
      "6168 598\n",
      "6172 600\n",
      "6176 601\n",
      "6180 601\n",
      "6184 601\n",
      "6188 602\n",
      "6192 602\n",
      "6196 602\n",
      "6200 602\n",
      "6204 603\n",
      "6208 603\n",
      "6212 603\n",
      "6216 603\n",
      "6220 603\n",
      "6224 604\n",
      "6228 604\n",
      "6232 605\n",
      "6236 606\n",
      "6240 606\n",
      "6244 607\n",
      "6248 607\n",
      "6252 609\n",
      "6256 611\n",
      "6260 612\n",
      "6264 613\n",
      "6268 613\n",
      "6272 614\n",
      "6276 615\n",
      "6280 615\n",
      "6284 616\n",
      "6288 616\n",
      "6292 616\n",
      "6296 616\n",
      "6300 618\n",
      "6304 618\n",
      "6308 618\n",
      "6312 619\n",
      "6316 620\n",
      "6320 620\n",
      "6324 620\n",
      "6328 620\n",
      "6332 620\n",
      "6336 620\n",
      "6340 621\n",
      "6344 621\n",
      "6348 621\n",
      "6352 621\n",
      "6356 622\n",
      "6360 622\n",
      "6364 623\n",
      "6368 623\n",
      "6372 623\n",
      "6376 624\n",
      "6380 624\n",
      "6384 625\n",
      "6388 626\n",
      "6392 627\n",
      "6396 627\n",
      "6400 628\n",
      "6404 628\n",
      "6408 628\n",
      "6412 628\n",
      "6416 628\n",
      "6420 628\n",
      "6424 628\n",
      "6428 628\n",
      "6432 628\n",
      "6436 628\n",
      "6440 629\n",
      "6444 630\n",
      "6448 630\n",
      "6452 630\n",
      "6456 630\n",
      "6460 632\n",
      "6464 632\n",
      "6468 633\n",
      "6472 633\n",
      "6476 633\n",
      "6480 633\n",
      "6484 633\n",
      "6488 633\n",
      "6492 634\n",
      "6496 634\n",
      "6500 634\n",
      "6504 634\n",
      "6508 634\n",
      "6512 634\n",
      "6516 636\n",
      "6520 637\n",
      "6524 637\n",
      "6528 637\n",
      "6532 637\n",
      "6536 637\n",
      "6540 638\n",
      "6544 638\n",
      "6548 639\n",
      "6552 639\n",
      "6556 639\n",
      "6560 639\n",
      "6564 639\n",
      "6568 639\n",
      "6572 640\n",
      "6576 641\n",
      "6580 641\n",
      "6584 641\n",
      "6588 642\n",
      "6592 642\n",
      "6596 642\n",
      "6600 642\n",
      "6604 642\n",
      "6608 643\n",
      "6612 644\n",
      "6616 644\n",
      "6620 644\n",
      "6624 644\n",
      "6628 645\n",
      "6632 645\n",
      "6636 645\n",
      "6640 646\n",
      "6644 646\n",
      "6648 647\n",
      "6652 647\n",
      "6656 648\n",
      "6660 648\n",
      "6664 648\n",
      "6668 649\n",
      "6672 650\n",
      "6676 650\n",
      "6680 651\n",
      "6684 651\n",
      "6688 652\n",
      "6692 652\n",
      "6696 652\n",
      "6700 652\n",
      "6704 652\n",
      "6708 652\n",
      "6712 653\n",
      "6716 653\n",
      "6720 653\n",
      "6724 654\n",
      "6728 654\n",
      "6732 654\n",
      "6736 655\n",
      "6740 655\n",
      "6744 656\n",
      "6748 657\n",
      "6752 657\n",
      "6756 657\n",
      "6760 657\n",
      "6764 658\n",
      "6768 658\n",
      "6772 659\n",
      "6776 659\n",
      "6780 660\n",
      "6784 661\n",
      "6788 661\n",
      "6792 661\n",
      "6796 661\n",
      "6800 661\n",
      "6804 661\n",
      "6808 661\n",
      "6812 661\n",
      "6816 661\n",
      "6820 661\n",
      "6824 661\n",
      "6828 662\n",
      "6832 662\n",
      "6836 662\n",
      "6840 663\n",
      "6844 663\n",
      "6848 663\n",
      "6852 663\n",
      "6856 663\n",
      "6860 664\n",
      "6864 664\n",
      "6868 664\n",
      "6872 665\n",
      "6876 665\n",
      "6880 665\n",
      "6884 666\n",
      "6888 666\n",
      "6892 667\n",
      "6896 667\n",
      "6900 667\n",
      "6904 667\n",
      "6908 667\n",
      "6912 667\n",
      "6916 667\n",
      "6920 667\n",
      "6924 668\n",
      "6928 668\n",
      "6932 671\n",
      "6936 671\n",
      "6940 671\n",
      "6944 671\n",
      "6948 672\n",
      "6952 672\n",
      "6956 672\n",
      "6960 672\n",
      "6964 672\n",
      "6968 672\n",
      "6972 672\n",
      "6976 672\n",
      "6980 672\n",
      "6984 672\n",
      "6988 672\n",
      "6992 672\n",
      "6996 673\n",
      "7000 673\n",
      "7004 674\n",
      "7008 674\n",
      "7012 674\n",
      "7016 675\n",
      "7020 675\n",
      "7024 676\n",
      "7028 676\n",
      "7032 676\n",
      "7036 676\n",
      "7040 676\n",
      "7044 676\n",
      "7048 676\n",
      "7052 678\n",
      "7056 679\n",
      "7060 680\n",
      "7064 680\n",
      "7068 680\n",
      "7072 681\n",
      "7076 681\n",
      "7080 681\n",
      "7084 682\n",
      "7088 683\n",
      "7092 684\n",
      "7096 685\n",
      "7100 686\n",
      "7104 686\n",
      "7108 687\n",
      "7112 688\n",
      "7116 689\n",
      "7120 689\n",
      "7124 689\n",
      "7128 690\n",
      "7132 690\n",
      "7136 690\n",
      "7140 691\n",
      "7144 692\n",
      "7148 692\n",
      "7152 693\n",
      "7156 693\n",
      "7160 694\n",
      "7164 695\n",
      "7168 695\n",
      "7172 695\n",
      "7176 695\n",
      "7180 695\n",
      "7184 695\n",
      "7188 696\n",
      "7192 697\n",
      "7196 697\n",
      "7200 697\n",
      "7204 697\n",
      "7208 699\n",
      "7212 700\n",
      "7216 700\n",
      "7220 701\n",
      "7224 701\n",
      "7228 701\n",
      "7232 702\n",
      "7236 703\n",
      "7240 703\n",
      "7244 704\n",
      "7248 704\n",
      "7252 704\n",
      "7256 706\n",
      "7260 706\n",
      "7264 707\n",
      "7268 709\n",
      "7272 709\n",
      "7276 710\n",
      "7280 710\n",
      "7284 711\n",
      "7288 712\n",
      "7292 713\n",
      "7296 713\n",
      "7300 713\n",
      "7304 713\n",
      "7308 714\n",
      "7312 714\n",
      "7316 715\n",
      "7320 715\n",
      "7324 716\n",
      "7328 716\n",
      "7332 716\n",
      "7336 716\n",
      "7340 716\n",
      "7344 717\n",
      "7348 717\n",
      "7352 718\n",
      "7356 718\n",
      "7360 718\n",
      "7364 719\n",
      "7368 721\n",
      "7372 722\n",
      "7376 722\n",
      "7380 722\n",
      "7384 722\n",
      "7388 722\n",
      "7392 722\n",
      "7396 723\n",
      "7400 724\n",
      "7404 724\n",
      "7408 724\n",
      "7412 724\n",
      "7416 725\n",
      "7420 725\n",
      "7424 726\n",
      "7428 726\n",
      "7432 728\n",
      "7436 729\n",
      "7440 729\n",
      "7444 729\n",
      "7448 729\n",
      "7452 729\n",
      "7456 729\n",
      "7460 730\n",
      "7464 730\n",
      "7468 730\n",
      "7472 730\n",
      "7476 731\n",
      "7480 731\n",
      "7484 731\n",
      "7488 731\n",
      "7492 732\n",
      "7496 734\n",
      "7500 734\n",
      "7504 734\n",
      "7508 734\n",
      "7512 735\n",
      "7516 735\n",
      "7520 735\n",
      "7524 735\n",
      "7528 735\n",
      "7532 737\n",
      "7536 737\n",
      "7540 737\n",
      "7544 737\n",
      "7548 738\n",
      "7552 738\n",
      "7556 738\n",
      "7560 741\n",
      "7564 742\n",
      "7568 742\n",
      "7572 742\n",
      "7576 742\n",
      "7580 743\n",
      "7584 743\n",
      "7588 744\n",
      "7592 746\n",
      "7596 746\n",
      "7600 746\n",
      "7604 747\n",
      "7608 747\n",
      "7612 747\n",
      "7616 747\n",
      "7620 747\n",
      "7624 747\n",
      "7628 749\n",
      "7632 749\n",
      "7636 749\n",
      "7640 749\n",
      "7644 749\n",
      "7648 750\n",
      "7652 751\n",
      "7656 751\n",
      "7660 751\n",
      "7664 751\n",
      "7668 753\n",
      "7672 754\n",
      "7676 755\n",
      "7680 756\n",
      "7684 756\n",
      "7688 756\n",
      "7692 756\n",
      "7696 756\n",
      "7700 757\n",
      "7704 757\n",
      "7708 757\n",
      "7712 758\n",
      "7716 759\n",
      "7720 759\n",
      "7724 760\n",
      "7728 760\n",
      "7732 760\n",
      "7736 760\n",
      "7740 761\n",
      "7744 763\n",
      "7748 765\n",
      "7752 765\n",
      "7756 765\n",
      "7760 765\n",
      "7764 766\n",
      "7768 766\n",
      "7772 766\n",
      "7776 766\n",
      "7780 766\n",
      "7784 768\n",
      "7788 768\n",
      "7792 768\n",
      "7796 768\n",
      "7800 768\n",
      "7804 769\n",
      "7808 770\n",
      "7812 770\n",
      "7816 771\n",
      "7820 771\n",
      "7824 772\n",
      "7828 773\n",
      "7832 774\n",
      "7836 774\n",
      "7840 775\n",
      "7844 775\n",
      "7848 775\n",
      "7852 775\n",
      "7856 775\n",
      "7860 776\n",
      "7864 776\n",
      "7868 776\n",
      "7872 776\n",
      "7876 776\n",
      "7880 777\n",
      "7884 777\n",
      "7888 777\n",
      "7892 777\n",
      "7896 777\n",
      "7900 777\n",
      "7904 778\n",
      "7908 779\n",
      "7912 779\n",
      "7916 780\n",
      "7920 780\n",
      "7924 780\n",
      "7928 780\n",
      "7932 782\n",
      "7936 782\n",
      "7940 783\n",
      "7944 785\n",
      "7948 786\n",
      "7952 786\n",
      "7956 786\n",
      "7960 786\n",
      "7964 786\n",
      "7968 786\n",
      "7972 786\n",
      "7976 786\n",
      "7980 787\n",
      "7984 788\n",
      "7988 789\n",
      "7992 789\n",
      "7996 789\n",
      "8000 790\n",
      "8004 790\n",
      "8008 790\n",
      "8012 792\n",
      "8016 793\n",
      "8020 794\n",
      "8024 796\n",
      "8028 797\n",
      "8032 798\n",
      "8036 798\n",
      "8040 798\n",
      "8044 799\n",
      "8048 800\n",
      "8052 800\n",
      "8056 800\n",
      "8060 801\n",
      "8064 802\n",
      "8068 802\n",
      "8072 802\n",
      "8076 802\n",
      "8080 802\n",
      "8084 802\n",
      "8088 802\n",
      "8092 802\n",
      "8096 802\n",
      "8100 803\n",
      "8104 803\n",
      "8108 803\n",
      "8112 803\n",
      "8116 803\n",
      "8120 803\n",
      "8124 803\n",
      "8128 803\n",
      "8132 804\n",
      "8136 804\n",
      "8140 804\n",
      "8144 804\n",
      "8148 804\n",
      "8152 804\n",
      "8156 804\n",
      "8160 804\n",
      "8164 805\n",
      "8168 806\n",
      "8172 806\n",
      "8176 808\n",
      "8180 808\n",
      "8184 809\n",
      "8188 809\n",
      "8192 809\n",
      "8196 809\n",
      "8200 809\n",
      "8204 810\n",
      "8208 810\n",
      "8212 810\n",
      "8216 810\n",
      "8220 811\n",
      "8224 811\n",
      "8228 811\n",
      "8232 812\n",
      "8236 813\n",
      "8240 813\n",
      "8244 813\n",
      "8248 813\n",
      "8252 814\n",
      "8256 815\n",
      "8260 815\n",
      "8264 815\n",
      "8268 816\n",
      "8272 817\n",
      "8276 818\n",
      "8280 819\n",
      "8284 819\n",
      "8288 820\n",
      "8292 821\n",
      "8296 822\n",
      "8300 823\n",
      "8304 824\n",
      "8308 824\n",
      "8312 825\n",
      "8316 825\n",
      "8320 827\n",
      "8324 827\n",
      "8328 827\n",
      "8332 828\n",
      "8336 828\n",
      "8340 828\n",
      "8344 829\n",
      "8348 829\n",
      "8352 829\n",
      "8356 829\n",
      "8360 829\n",
      "8364 829\n",
      "8368 829\n",
      "8372 829\n",
      "8376 829\n",
      "8380 829\n",
      "8384 829\n",
      "8388 830\n",
      "8392 830\n",
      "8396 830\n",
      "8400 830\n",
      "8404 830\n",
      "8408 831\n",
      "8412 831\n",
      "8416 832\n",
      "8420 832\n",
      "8424 833\n",
      "8428 833\n",
      "8432 833\n",
      "8436 834\n",
      "8440 834\n",
      "8444 834\n",
      "8448 834\n",
      "8452 834\n",
      "8456 835\n",
      "8460 835\n",
      "8464 836\n",
      "8468 837\n",
      "8472 838\n",
      "8476 839\n",
      "8480 840\n",
      "8484 841\n",
      "8488 841\n",
      "8492 842\n",
      "8496 842\n",
      "8500 842\n",
      "8504 842\n",
      "8508 843\n",
      "8512 843\n",
      "8516 843\n",
      "8520 843\n",
      "8524 843\n",
      "8528 843\n",
      "8532 843\n",
      "8536 843\n",
      "8540 843\n",
      "8544 844\n",
      "8548 844\n",
      "8552 844\n",
      "8556 844\n",
      "8560 845\n",
      "8564 846\n",
      "8568 846\n",
      "8572 846\n",
      "8576 846\n",
      "8580 848\n",
      "8584 848\n",
      "8588 848\n",
      "8592 849\n",
      "8596 849\n",
      "8600 850\n",
      "8604 850\n",
      "8608 850\n",
      "8612 851\n",
      "8616 851\n",
      "8620 851\n",
      "8624 851\n",
      "8628 852\n",
      "8632 852\n",
      "8636 852\n",
      "8640 852\n",
      "8644 852\n",
      "8648 853\n",
      "8652 854\n",
      "8656 855\n",
      "8660 855\n",
      "8664 855\n",
      "8668 855\n",
      "8672 855\n",
      "8676 855\n",
      "8680 855\n",
      "8684 855\n",
      "8688 856\n",
      "8692 856\n",
      "8696 856\n",
      "8700 857\n",
      "8704 857\n",
      "8708 857\n",
      "8712 857\n",
      "8716 857\n",
      "8720 858\n",
      "8724 859\n",
      "8728 859\n",
      "8732 861\n",
      "8736 863\n",
      "8740 864\n",
      "8744 865\n",
      "8748 866\n",
      "8752 867\n",
      "8756 867\n",
      "8760 868\n",
      "8764 868\n",
      "8768 868\n",
      "8772 869\n",
      "8776 869\n",
      "8780 869\n",
      "8784 869\n",
      "8788 869\n",
      "8792 870\n",
      "8796 870\n",
      "8800 870\n",
      "8804 871\n",
      "8808 871\n",
      "8812 872\n",
      "8816 873\n",
      "8820 873\n",
      "8824 874\n",
      "8828 874\n",
      "8832 874\n",
      "8836 874\n",
      "8840 874\n",
      "8844 875\n",
      "8848 875\n",
      "8852 875\n",
      "8856 875\n",
      "8860 875\n",
      "8864 876\n",
      "8868 876\n",
      "8872 877\n",
      "8876 878\n",
      "8880 878\n",
      "8884 878\n",
      "8888 878\n",
      "8892 878\n",
      "8896 878\n",
      "8900 878\n",
      "8904 880\n",
      "8908 880\n",
      "8912 880\n",
      "8916 880\n",
      "8920 880\n",
      "8924 880\n",
      "8928 881\n",
      "8932 881\n",
      "8936 881\n",
      "8940 881\n",
      "8944 881\n",
      "8948 881\n",
      "8952 881\n",
      "8956 882\n",
      "8960 883\n",
      "8964 883\n",
      "8968 883\n",
      "8972 883\n",
      "8976 885\n",
      "8980 885\n",
      "8984 887\n",
      "8988 887\n",
      "8992 888\n",
      "8996 888\n",
      "9000 890\n",
      "9004 891\n",
      "9008 892\n",
      "9012 892\n",
      "9016 892\n",
      "9020 892\n",
      "9024 892\n",
      "9028 893\n",
      "9032 894\n",
      "9036 895\n",
      "9040 896\n",
      "9044 899\n",
      "9048 900\n",
      "9052 900\n",
      "9056 901\n",
      "9060 901\n",
      "9064 901\n",
      "9068 901\n",
      "9072 902\n",
      "9076 902\n",
      "9080 902\n",
      "9084 903\n",
      "9088 904\n",
      "9092 906\n",
      "9096 907\n",
      "9100 907\n",
      "9104 907\n",
      "9108 907\n",
      "9112 908\n",
      "9116 908\n",
      "9120 908\n",
      "9124 908\n",
      "9128 908\n",
      "9132 909\n",
      "9136 910\n",
      "9140 910\n",
      "9144 910\n",
      "9148 910\n",
      "9152 910\n",
      "9156 911\n",
      "9160 911\n",
      "9164 911\n",
      "9168 911\n",
      "9172 912\n",
      "9176 912\n",
      "9180 912\n",
      "9184 912\n",
      "9188 912\n",
      "9192 913\n",
      "9196 914\n",
      "9200 914\n",
      "9204 914\n",
      "9208 914\n",
      "9212 914\n",
      "9216 914\n",
      "9220 914\n",
      "9224 914\n",
      "9228 915\n",
      "9232 916\n",
      "9236 917\n",
      "9240 917\n",
      "9244 918\n",
      "9248 918\n",
      "9252 918\n",
      "9256 919\n",
      "9260 920\n",
      "9264 921\n",
      "9268 921\n",
      "9272 922\n",
      "9276 922\n",
      "9280 922\n",
      "9284 922\n",
      "9288 922\n",
      "9292 923\n",
      "9296 924\n",
      "9300 924\n",
      "9304 924\n",
      "9308 925\n",
      "9312 926\n",
      "9316 926\n",
      "9320 927\n",
      "9324 928\n",
      "9328 928\n",
      "9332 928\n",
      "9336 928\n",
      "9340 929\n",
      "9344 929\n",
      "9348 930\n",
      "9352 930\n",
      "9356 931\n",
      "9360 932\n",
      "9364 932\n",
      "9368 933\n",
      "9372 934\n",
      "9376 935\n",
      "9380 935\n",
      "9384 935\n",
      "9388 935\n",
      "9392 935\n",
      "9396 935\n",
      "9400 935\n",
      "9404 935\n",
      "9408 935\n",
      "9412 935\n",
      "9416 935\n",
      "9420 935\n",
      "9424 935\n",
      "9428 937\n",
      "9432 938\n",
      "9436 938\n",
      "9440 938\n",
      "9444 938\n",
      "9448 938\n",
      "9452 938\n",
      "9456 939\n",
      "9460 939\n",
      "9464 940\n",
      "9468 941\n",
      "9472 941\n",
      "9476 941\n",
      "9480 942\n",
      "9484 943\n",
      "9488 943\n",
      "9492 943\n",
      "9496 943\n",
      "9500 943\n",
      "9504 943\n",
      "9508 943\n",
      "9512 943\n",
      "9516 943\n",
      "9520 943\n",
      "9524 943\n",
      "9528 943\n",
      "9532 943\n",
      "9536 943\n",
      "9540 943\n",
      "9544 943\n",
      "9548 943\n",
      "9552 944\n",
      "9556 944\n",
      "9560 945\n",
      "9564 945\n",
      "9568 945\n",
      "9572 945\n",
      "9576 945\n",
      "9580 945\n",
      "9584 945\n",
      "9588 945\n",
      "9592 946\n",
      "9596 946\n",
      "9600 946\n",
      "9604 946\n",
      "9608 947\n",
      "9612 947\n",
      "9616 947\n",
      "9620 947\n",
      "9624 947\n",
      "9628 948\n",
      "9632 948\n",
      "9636 949\n",
      "9640 951\n",
      "9644 951\n",
      "9648 952\n",
      "9652 953\n",
      "9656 953\n",
      "9660 954\n",
      "9664 955\n",
      "9668 955\n",
      "9672 955\n",
      "9676 955\n",
      "9680 955\n",
      "9684 955\n",
      "9688 956\n",
      "9692 957\n",
      "9696 957\n",
      "9700 957\n",
      "9704 957\n",
      "9708 958\n",
      "9712 958\n",
      "9716 958\n",
      "9720 958\n",
      "9724 958\n",
      "9728 958\n",
      "9732 959\n",
      "9736 959\n",
      "9740 960\n",
      "9744 961\n",
      "9748 962\n",
      "9752 963\n",
      "9756 964\n",
      "9760 964\n",
      "9764 965\n",
      "9768 966\n",
      "9772 966\n",
      "9776 966\n",
      "9780 967\n",
      "9784 968\n",
      "9788 968\n",
      "9792 968\n",
      "9796 968\n",
      "9800 968\n",
      "9804 968\n",
      "9808 968\n",
      "9812 968\n",
      "9816 968\n",
      "9820 969\n",
      "9824 969\n",
      "9828 969\n",
      "9832 970\n",
      "9836 971\n",
      "9840 973\n",
      "9844 973\n",
      "9848 973\n",
      "9852 974\n",
      "9856 975\n",
      "9860 975\n",
      "9864 975\n",
      "9868 975\n",
      "9872 976\n",
      "9876 977\n",
      "9880 977\n",
      "9884 977\n",
      "9888 977\n",
      "9892 977\n",
      "9896 977\n",
      "9900 977\n",
      "9904 977\n",
      "9908 977\n",
      "9912 977\n",
      "9916 977\n",
      "9920 977\n",
      "9924 977\n",
      "9928 977\n",
      "9932 977\n",
      "9936 977\n",
      "9940 977\n",
      "9944 977\n",
      "9948 978\n",
      "9952 978\n",
      "9956 978\n",
      "9960 978\n",
      "9964 979\n",
      "9968 981\n",
      "9972 982\n",
      "9976 982\n",
      "9980 984\n",
      "9984 985\n",
      "9988 986\n",
      "9992 986\n",
      "9996 987\n",
      "10000 988\n",
      "Accuracy of the network on the 10000 test images: 9.88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # print(total,correct)\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff57ed-bb03-42cd-9446-a36e8f1f26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in target_names}\n",
    "total_pred = {classname: 0 for classname in target_names}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[target_names[label]] += 1\n",
    "            total_pred[target_names[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae1b24-f678-493f-a657-aa4dc00afc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4f86a-90b9-4ed8-b78a-6c07946850af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7292ab7e-c500-4a3b-a8cd-1bcf51fd6c5c",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "\n",
    "In this assignment, we Use the convolutional neural network dicsussed in Lab 5 to address the following questions.\n",
    "\n",
    "1. Implement k-fold cross-validation with k=5 and report the results including average and standard deviaion of accuracy, f1-score, sensitivity, specificity as $\\%xx.x\\pm xx.x$ e.g. $\\% 65.1\\pm 2.1$\n",
    "\n",
    "2. Plot the training and validation loss and accuracy curves (for each epoch) in a single figure and discuss your observation with respect to the training performance. Do you observe under-fitting and/or over-fitting? If so, define in wich epochs. \n",
    "\n",
    "3. Implament two regularization methods to prevent over-fitting in training the model. Describe how these approaches can reduce over-fitting.\n",
    "Report the results according to 1.\n",
    "\n",
    "4. Propose, describe, and implement two approaches (not including regularization) to enhance the classificaton performance fo the model. Report the results according to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62410f68-4e56-4eec-8861-994ee623d682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a86d90-313a-406d-a885-e876d75eabbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
